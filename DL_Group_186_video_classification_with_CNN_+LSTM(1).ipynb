{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Group 186 - Assignment 2 - Video Classification using CNN+LSTM \n",
        "\n",
        "In this Group 186 Assignment exercise, we will implement human activity recognition on videos using a Convolutional Neural Network combined with a Long-Short Term Memory Network, we’ll be using different architectures that are created in TensorFlow. We'll be doing Video Classification in order to perform activity recognition.\n",
        "\n",
        "## Group No 186\n",
        "\n",
        "## Group Member Names:\n",
        "1. Sindhu C - 2021FC04993@wilp.bits-pilani.ac.in\n",
        "2. Lajish VL - 2021fc04980@wilp.bits-pilani.ac.in\n",
        "3. Sivarajan N - 2021fc04989@wilp.bits-pilani.ac.in\n"
      ],
      "metadata": {
        "id": "yuMF3R8DA5CX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video Classification with a CNN+LSTM Architecture\n",
        "\n",
        "We would follow the steps below for this task.\n",
        "\n",
        "◆ Data collection\n",
        "\n",
        "◆ Setup\n",
        "\n",
        "◆ Define hyperparameters\n",
        "\n",
        "◆ Data preparation\n",
        "\n",
        "◆ The sequence model\n",
        "\n",
        "◆ Evaluation and Inference "
      ],
      "metadata": {
        "id": "taXI98JTnu9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step i: Import Libraries/Dataset\n",
        "\n",
        "```\n",
        "• Import the required libraries.\n",
        "• Check the GPU available (recommended- use free GPU provided by Google Colab).\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lnnlQS3o0xQ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yr976LVjaPZX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the required libraries. silent mode, no output required\n",
        "!pip install moviepy imageio==2.4.1 -q gwpy\n",
        "!pip install git+https://github.com/tensorflow/docs -q gwpy\n",
        "!pip install tensorflow-gpu -q gwpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries.\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import timeit\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "from moviepy.editor import *\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "import tensorflow_docs\n",
        "from tensorflow_docs.vis import embed \n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "YhNDivwtaRky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef550da-41a4-46cf-cc35-9195f4f762c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.3.0 (SDL 2.24.2, Python 3.9.16)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMce8muBqXQP"
      },
      "source": [
        "### Check Tensorflow with GPU\n",
        "\n",
        "```\n",
        "The next step provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. \n",
        "\n",
        "We will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU\n",
        "And then observe the speedup provided by using the GPU.\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  To Enable GPUs for the notebook:\n",
        "#  Navigate to Colab Menu->Edit→Notebook Settings\n",
        "#  Select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "# Next, we'll confirm that we can connect to the GPU with tensorflow:\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n",
        "\n",
        "# Set Tensorflor to use GPU\n",
        "# Let TensorFlow to automatically choose an existing and supported device to run the operations \n",
        "# And in case the specified one doesn't exist, you can call tf.config.set_soft_device_placement(True).\n",
        "tf.device('/device:GPU:0')\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false'\n",
        "tf.config.set_soft_device_placement(True)\n"
      ],
      "metadata": {
        "id": "fY-yQpjy34Tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30190f49-4c7b-43e1-e113-a77b4e078a4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "8.67597305199979\n",
            "GPU (s):\n",
            "0.14368549199980407\n",
            "GPU speedup over CPU: 60x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above speedup was seen by Enabling GPU over CPU 30x to 60x times."
      ],
      "metadata": {
        "id": "ttbH3pin7AwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step ii. Data Processing\n",
        "\n",
        "• Download the data from https://www.crcv.ucf.edu/data/UCF50.rar, extract the dataset\n",
        "\n",
        "• Convert the data into the correct format which could be used for the DL model.\n",
        "\n",
        "• Plot at least two (we did 10) samples and their captions (use matplotlib/seaborn/any other library - we used matplotlib).\n",
        "\n",
        "• Load the data into train and test data in the required format."
      ],
      "metadata": {
        "id": "rksOkyy74OPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data if not already downloaded and extract the dataset.\n",
        "%%capture\n",
        "\n",
        "# Downlaod the UCF50 Dataset\n",
        "# UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. \n",
        "# UCF101 data set is an extension of UCF50 data set which has 50 action categories. \n",
        "!wget -nc \"https://www.crcv.ucf.edu/data/UCF50.rar\"\n",
        "\n",
        "#Extract the Dataset\n",
        "!unrar x UCF50.rar\n"
      ],
      "metadata": {
        "id": "S_yfBlgSawYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the downloaded file, and extracted folders\n",
        "!pwd\n",
        "!ls -ll\n",
        "!ls UCF50 -ll"
      ],
      "metadata": {
        "id": "2ERtpU9ZSgOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's set the Seed values so every run is consistent.\n",
        "seed_constant = 27\n",
        "np.random.seed(seed_constant)\n",
        "random.seed(seed_constant)\n",
        "tf.random.set_seed(seed_constant)"
      ],
      "metadata": {
        "id": "olXww8FPX1E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Plot 10 samples and their captions using Matplotlib\n",
        "# Create a Matplotlib figure and specify the size of the figure.\n",
        "plt.figure(figsize = (20, 20))\n",
        "\n",
        "# Get the names of all classes/categories in UCF50.\n",
        "all_classes_names = os.listdir('UCF50')\n",
        "\n",
        "# Generate a list of 10 random values. The values will be between 0-50, \n",
        "# where 50 is the total number of class in the dataset. \n",
        "random_range = random.sample(range(len(all_classes_names)), 10)\n",
        "\n",
        "# Iterating through all the generated random values.\n",
        "for counter, random_index in enumerate(random_range, 1):\n",
        "\n",
        "    # Retrieve a Class Name using the Random Index.\n",
        "    selected_class_Name = all_classes_names[random_index]\n",
        "\n",
        "    # Retrieve the list of all the video files present in the randomly selected Class Directory.\n",
        "    video_files_names_list = os.listdir(f'UCF50/{selected_class_Name}')\n",
        "\n",
        "    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.\n",
        "    selected_video_file_name = random.choice(video_files_names_list)\n",
        "\n",
        "    # We will be using OpenCV's VideoCapture() method to read frames from videos.\n",
        "    # Initialize a VideoCapture object to read from the video File.\n",
        "    video_reader = cv2.VideoCapture(f'UCF50/{selected_class_Name}/{selected_video_file_name}')\n",
        "    \n",
        "    # Read the first frame of the video file.\n",
        "    _, bgr_frame = video_reader.read()\n",
        "\n",
        "    # Release the VideoCapture object. \n",
        "    video_reader.release()\n",
        "\n",
        "    # Convert the frame from BGR into RGB format. \n",
        "    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Write the class name on the video frame.\n",
        "    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "    \n",
        "    # Display the frame.\n",
        "    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')"
      ],
      "metadata": {
        "id": "if9gA-VQaxv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the height and width to which each video frame will be resized in our dataset.\n",
        "IMAGE_HEIGHT , IMAGE_WIDTH, IMG_SIZE = 224, 224, 224\n",
        "\n",
        "# Specify the number of frames of a video that will be fed to the model as one sequence.\n",
        "SEQUENCE_LENGTH = 5\n",
        "\n",
        "# Specifying the directory containing the UCF50 dataset. \n",
        "DATASET_DIR = \"UCF50\"\n",
        "\n",
        "# Specifying the list containing the names of the classes used for training.  \n",
        "# we pick 5 random class names from the test data set, which will also be used in Step vi for Prediction.\n",
        "n=5\n",
        "CLASSES_LIST=random.choices(all_classes_names, k=n)\n",
        "#CLASSES_LIST = [\"VolleyballSpiking\",\"PushUps\",\"PullUps\"]\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 2048"
      ],
      "metadata": {
        "id": "vsUky5j2a0JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frames_extraction(video_path):\n",
        "    '''\n",
        "    This function will extract the required frames from a video after resizing and normalizing them.\n",
        "    Args:\n",
        "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
        "    Returns:\n",
        "        frames_list: A list containing the resized and normalized frames of the video.\n",
        "    '''\n",
        "\n",
        "    # Declare a list to store video frames.\n",
        "    frames_list = []\n",
        "    \n",
        "    # Read the Video File using the VideoCapture object.\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get the total number of frames in the video.\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the the interval after which frames will be added to the list.\n",
        "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
        "\n",
        "    # Iterate through the Video Frames.\n",
        "    for frame_counter in range(SEQUENCE_LENGTH):\n",
        "\n",
        "        # Set the current frame position of the video.\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "\n",
        "        # Reading the frame from the video. \n",
        "        success, frame = video_reader.read() \n",
        "\n",
        "        # Check if Video frame is not successfully read then break the loop\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        # Resize the Frame to fixed height and width.\n",
        "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        \n",
        "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
        "        normalized_frame = resized_frame / 255\n",
        "        \n",
        "        # Append the normalized frame into the frames list\n",
        "        frames_list.append(normalized_frame)\n",
        "    \n",
        "    # Release the VideoCapture object. \n",
        "    video_reader.release()\n",
        "\n",
        "    # Return the frames list.\n",
        "    return frames_list"
      ],
      "metadata": {
        "id": "DZXGoBbka4PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_dataset():\n",
        "    '''\n",
        "    This function will extract the data of the selected classes and create the required dataset.\n",
        "    Returns:\n",
        "        features:          A list containing the extracted frames of the videos.\n",
        "        labels:            A list containing the indexes of the classes associated with the videos.\n",
        "        video_files_paths: A list containing the paths of the videos in the disk.\n",
        "    '''\n",
        "\n",
        "    # Declared Empty Lists to store the features, labels and video file path values.\n",
        "    features = []\n",
        "    labels = []\n",
        "    video_files_paths = []\n",
        "    \n",
        "    # Iterating through all the classes mentioned in the classes list\n",
        "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
        "        \n",
        "        # Display the name of the class whose data is being extracted.\n",
        "        print(f'Extracting Data of Class: {class_name}')\n",
        "        \n",
        "        # Get the list of video files present in the specific class name directory.\n",
        "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
        "        \n",
        "        # Iterate through all the files present in the files list.\n",
        "        for file_name in files_list:\n",
        "            \n",
        "            # Get the complete video path.\n",
        "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
        "\n",
        "            # Extract the frames of the video file.\n",
        "            frames = frames_extraction(video_file_path)\n",
        "\n",
        "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
        "            # So ignore the videos having frames less than the SEQUENCE_LENGTH.\n",
        "            if len(frames) == SEQUENCE_LENGTH:\n",
        "\n",
        "                # Append the data to their repective lists.\n",
        "                features.append(frames)\n",
        "                labels.append(class_index)\n",
        "                video_files_paths.append(video_file_path)\n",
        "\n",
        "    # Converting the list to numpy arrays\n",
        "    features = np.asarray(features)\n",
        "    labels = np.array(labels)  \n",
        "    \n",
        "    # Return the frames, class index, and video file path.\n",
        "    return features, labels, video_files_paths"
      ],
      "metadata": {
        "id": "YOYDpcIPa8oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset.\n",
        "features, labels, video_files_paths = create_dataset()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "Rzf8ZiMMa9xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\n",
        "one_hot_encoded_labels = to_categorical(labels)"
      ],
      "metadata": {
        "id": "wtNgsKFLbCkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step ii: Load the data into train and test data in the required format.\n",
        "# Split the Data into Train ( 75% ) and Test Set ( 25% ).\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.25, shuffle = True, random_state = seed_constant)"
      ],
      "metadata": {
        "id": "3y6-8kIdbE4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step iii. Model Building\n",
        "\n",
        "• Use any pre-trained model trained on ImageNet dataset (available publicly on Google) for image feature extraction.\n",
        "\n",
        "• Create k-layered LSTM model and other relevant layers.\n",
        "\n",
        "• Add one layer of dropout at the appropriate position and give reasons.\n",
        "\n",
        "• Choose the appropriate activation function for all the layers.\n",
        "\n",
        "• Print the model summary.\n",
        "\n",
        "• Justify the choice of number of layers, activation function and any other hyper parameters used."
      ],
      "metadata": {
        "id": "Etcx17Td55w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function will construct the required convlstm model.\n",
        "def create_convlstm_model():\n",
        "\n",
        "    # We will use a Sequential model for model construction\n",
        "    model = Sequential()\n",
        "\n",
        "    # Define the Model Architecture.\n",
        "    ########################################################################################################################\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,\n",
        "                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    #model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(Flatten()) \n",
        "    \n",
        "    model.add(Dense(len(CLASSES_LIST), activation = \"softmax\"))\n",
        "    \n",
        "    ########################################################################################################################\n",
        "     \n",
        "    # Display the models summary.\n",
        "    model.summary()\n",
        "    \n",
        "    # Return the constructed convlstm model.\n",
        "    return model"
      ],
      "metadata": {
        "id": "7OhWhBLybH9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the required convlstm model.\n",
        "convlstm_model = create_convlstm_model()\n",
        "\n",
        "# Display the success message. \n",
        "print(\"Model Created Successfully!\")"
      ],
      "metadata": {
        "id": "iMOhOqyObNhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the structure of the contructed model.\n",
        "plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)\n"
      ],
      "metadata": {
        "id": "-uCgnR83bRRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Justification for the choice of number of layers, activation function used  and Hyper Parameters\n",
        "\n",
        "The approach we will implement in this Group 186 Assignment to build an Action Recognizer - we will use a Convolution Neural Network (CNN) + Long Short Term Memory (LSTM) Network to perform Action Recognition while utilizing the Spatial-temporal aspect of the videos.\n",
        "\n",
        "#### Number of Layers: \n",
        "* To construct the model, we will use Keras **ConvLSTM2D recurrent** layers. The ConvLSTM2D layer also takes in the number of filters and kernel size required for applying the convolutional operations. The output of the layers is flattened in the end and is fed to the Dense layer with softmax activation which outputs the probability of each action category.\n",
        "\n",
        "* We will also use **MaxPooling3D layers** to reduce the dimensions of the frames and avoid unnecessary computations and Dropout layers to prevent overfitting the model on the data. \n",
        "\n",
        "#### Activation Function - softmax\n",
        "* The output of the layers is flattened in the end and is fed to the Dense layer with **softmax** activation which outputs the probability of each action category.\n",
        "* This architecture is a simple one and has a small number of trainable parameters. This is because we are only dealing with a small subset of the dataset which does not require a large-scale model.\n",
        "\n",
        "#### Model Choice\n",
        "* We use an **LSTM network**  as it is specifically designed to work with a data sequence as it takes into consideration all of the previous inputs while generating an output. Other choice would be to use RNN. \n",
        "* We will use a **Sequential** model for model construction. This makes an LSTM more capable of solving problems involving sequential data such as time series prediction, speech recognition, language translation, or music composition. But for now, we will only explore the role of LSTMs in developing better action recognition models. Other choice for model would be Long-term Recurrent Convolutional Network (LRCN), which combines CNN and LSTM layers in a single model.\n",
        "\n"
      ],
      "metadata": {
        "id": "8WIpg4YX_cAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step iv. Model Compilation \n",
        "\n",
        "• Compile the model with the appropriate loss function.\n",
        "\n",
        "• Use an appropriate optimizer - we used Adam\n",
        "\n",
        "• Justify the choice of learning rate, optimizer, loss function and any other hyper parameter used."
      ],
      "metadata": {
        "id": "TuvDzS0W6Qku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Instance of Early Stopping Callback\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)\n",
        "print(f\"Created Early Stopping Callback\")\n",
        "# Cleanup garbage\n",
        "gc.collect()\n",
        "# Compile the model and specify loss function, optimizer and metrics values to the model\n",
        "convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "OYFTM5e_bVP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Justification for the Model choices and Hyper Parameters:\n",
        "#### Loss function\n",
        "*   Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "*   In this case, we can see the model performed well, achieving a classification accuracy of about 74% on the training dataset.\n",
        "\n",
        "#### Optimizer\n",
        "*   The results of the Adam optimizer are generally better than every other optimization algorithm, have faster computation time, and require fewer parameters for tuning.\n",
        "\n",
        "#### Metrics \n",
        "* A metric is a function that is used to judge the performance of your model. **Accuracy** is a metric that generally describes how the model performs across all classes. It is useful when all classes are of equal importance. It is calculated as the ratio between the number of correct predictions to the total number of predictions. \n"
      ],
      "metadata": {
        "id": "3QKr3rq64REC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step v. Model Training\n",
        "\n",
        "• Train the model for an appropriate number of epochs - we chose 50 epochs\n",
        "\n",
        "• Print the train and validation loss for each epoch. Use the appropriate batch size.\n",
        "\n",
        "• Plot the loss and accuracy history graphs for both train and validation set.\n",
        "\n",
        "• Print the total time taken for training."
      ],
      "metadata": {
        "id": "hyOsHEv26u2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start training the model.\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 10,shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])\n",
        "end = time.time()\n",
        "total_time = end-start\n",
        "print (\"Total time taken for training (in seconds): \", (\"%.2f\" % total_time))\n"
      ],
      "metadata": {
        "id": "XMBdWkWI7vNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model.\n",
        "model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)"
      ],
      "metadata": {
        "id": "pHyHsEf1bmoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the loss and accuracy from model_evaluation_history.\n",
        "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
        "\n",
        "# Define the string date format.\n",
        "# Get the current Date and Time in a DateTime Object.\n",
        "# Convert the DateTime object to string according to the style mentioned in date_time_format string.\n",
        "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
        "current_date_time_dt = dt.datetime.now()\n",
        "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
        "\n",
        "# Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n",
        "model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
        "\n",
        "# Save your Model.\n",
        "convlstm_model.save(model_file_name)"
      ],
      "metadata": {
        "id": "NuyODD7WbrGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n",
        "    '''\n",
        "    This function will plot the metrics passed to it in a graph.\n",
        "    Args:\n",
        "        model_training_history: A history object containing a record of training and validation \n",
        "                                loss values and metrics values at successive epochs\n",
        "        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n",
        "        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n",
        "        plot_name:              The title of the graph.\n",
        "    '''\n",
        "    \n",
        "    # Get metric values using metric names as identifiers.\n",
        "    metric_value_1 = model_training_history.history[metric_name_1]\n",
        "    metric_value_2 = model_training_history.history[metric_name_2]\n",
        "    \n",
        "    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n",
        "    epochs = range(len(metric_value_1))\n",
        "\n",
        "    # Plot the Graph.\n",
        "    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n",
        "    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n",
        "\n",
        "    # Add title to the plot.\n",
        "    plt.title(str(plot_name))\n",
        "\n",
        "    # Add legend to the plot.\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "4hO5slfdbtrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training and validation loss metrices.\n",
        "plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"
      ],
      "metadata": {
        "id": "3XFbfJXbbvm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training and validation accuracy metrices.\n",
        "plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy') "
      ],
      "metadata": {
        "id": "EuiLghJ9bzLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step vi. Model Evaluation\n",
        "\n",
        "• Take 5 random data from the test set and perform activity recognition.\n",
        "\n",
        "• Print confusion metrics and classification report for the test data."
      ],
      "metadata": {
        "id": "c55JsNF5AObq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To perform Activity recognition \n",
        "``\n",
        "# Lets get 5 Random data from the Test set\n",
        "n=5\n",
        "random_names=random.choices(all_classes_names, k=n)\n",
        "print (f\"5 Random Data from the test set: \",random_names)\n",
        "``\n",
        "# Get the labels for the 5 Classes from dataset set up earlier \n",
        "from tensorflow import keras\n",
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(CLASSES_LIST)\n",
        ")\n",
        "label_processor.get_vocabulary()\n",
        "\n"
      ],
      "metadata": {
        "id": "Gp70G9tMk_By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract Features from a video file.\n",
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n"
      ],
      "metadata": {
        "id": "5GnF-09jxb0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the activity from the frame grab\n",
        "\n",
        "def prepare_single_video(frames):\n",
        "    frames = frames[None, ...]\n",
        "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "#    probabilities = convlstm_model.predict([frame_features, frame_mask])[0]\n",
        "\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "    return frame_features, frame_mask\n",
        "\n",
        "\n",
        "def sequence_prediction(path):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = np.array(frames_extraction (video_files_paths[0]))\n",
        "    #frame_features, frame_mask = prepare_single_video(frames)\n",
        "    probabilities = convlstm_model.predict(np.expand_dims(frames, axis=0))\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        #print(probabilities[i])\n",
        "        print(f\"   {probabilities * 100}%\") # correct the format to show the right probability\n",
        "    return frames\n",
        "\n",
        "\n",
        "# This utility is for visualization, referenced from:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def to_gif(images):\n",
        "    converted_images = images.astype(np.uint8)\n",
        "    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
        "    return embed.embed_file(\"animation.gif\")\n",
        "\n",
        "\n",
        "#test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
        "for vdo in video_files_paths:\n",
        "  print(f\"Test video path: {vdo}\")\n",
        "  test_frames = sequence_prediction(vdo)\n",
        "# to_gif(test_frames[:MAX_SEQ_LENGTH])\n"
      ],
      "metadata": {
        "id": "MZI1F_YPESDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets get the Confusion matrix and classification report printed.\n",
        "frames = np.array(frames_extraction (video_files_paths[0]))\n",
        "predicted = convlstm_model.predict(np.expand_dims(frames, axis=0))\n",
        "y_pred = predicted.argmax(axis=1)\n",
        "#print(classification_report(labels_test.argmax(axis=1), y_pred, target_names=target_names))\n",
        "print(confusion_matrix(labels_test, y_pred))"
      ],
      "metadata": {
        "id": "C9xhiJI1DO74"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}