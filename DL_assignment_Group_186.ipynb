{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bda93ba",
   "metadata": {
    "id": "9bda93ba"
   },
   "source": [
    "## Group No 186\n",
    "\n",
    "## Group Member Names:\n",
    "1. Sindhu C - 2021FC04993@wilp.bits-pilani.ac.in\n",
    "2. Lajish VL - 2021fc04980@wilp.bits-pilani.ac.in\n",
    "3. Sivarajan N - 2021fc04989@wilp.bits-pilani.ac.in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d80c60",
   "metadata": {
    "id": "f5d80c60"
   },
   "source": [
    "# 1. Problem Statement\n",
    " \n",
    "Students are expected to identify a classification / regression problem of your choice. You have to detail the problem under this heading which basically addresses the following questions.\n",
    " \n",
    "   1. What is the problem that you are trying to solve?\n",
    "    ## **Customer Churn Modelling** - Finance problem for Banks.\n",
    " - This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account/exited) or he continues to be a customer.\n",
    " - In this problem, we predict if a customer is likely to leave or unsubscribe from the bank service.\n",
    "\n",
    "   2. What kind of prediction (classification / regression) task are you performing?\n",
    "   - We are using **binary classification problem** in this problem where we will be having only two classes as output (1 and 0). Here the activation function  used is sigmoid. \n",
    "\n",
    "ENSURE THAT YOU ARE USING NUMERICAL / CATEGORICAL DATA only - we have used Numerical data only here.\n",
    "\n",
    "DO NOT use images or textual data.\n",
    "\n",
    "Score: 1 Mark in total (0.5 mark each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8e0cb",
   "metadata": {
    "id": "3cc8e0cb"
   },
   "source": [
    "# 2. Data Acquisition\n",
    " \n",
    "For the problem identified by you, students have to find the data source themselves from any data source.\n",
    " - Customer Churn data set is loaded into our private Github repository and read from there.\n",
    "-  https://raw.githubusercontent.com/ns-rajan/Collab/main/churns.csv\n",
    "\n",
    "\n",
    "## 2.1 Download the data directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b51d895",
   "metadata": {
    "id": "4b51d895"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30412/3203396305.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Import libraries for Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "#Import Libraries required for this DL Assignment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import libraries for visualization and reporting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import libraries for Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O5Y8gf2OMIvz",
   "metadata": {
    "id": "O5Y8gf2OMIvz"
   },
   "outputs": [],
   "source": [
    "#Download and Load Dataset from the Github Repo\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/ns-rajan/Collab/main/churns.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49530d0c",
   "metadata": {
    "id": "49530d0c"
   },
   "source": [
    "## 2.2 Code for converting the above downloaded data into a form suitable for DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4c171",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "c1f4c171",
    "outputId": "1c330b47-7cdd-42ae-becc-0e7eae6e6f3d"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "## Check the downloaded data, and summarize it \n",
    "data.shape\n",
    "\n",
    "data.info()\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812edb18",
   "metadata": {
    "id": "812edb18"
   },
   "source": [
    "## 2.3 Write your observations from the above. \n",
    "\n",
    "1. Size of the dataset - **the data set has 10000 rows, 14 dimensions.**\n",
    "2. What type of data attributes are there? - **HasCrCard, IsActiveMember & Exited - have Binary Data. Definition and data type of all 14 dimensions given below.**\n",
    "\n",
    "Score: 2 Mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UXSVgM-UCD2I",
   "metadata": {
    "id": "UXSVgM-UCD2I"
   },
   "source": [
    "\n",
    "\n",
    "## 2.3.1 - Definition and data type of the 14 dimensions\n",
    "    - RowNumber  - number of the row, Integer\n",
    "    - CustomerId - unique ID that identifies each customer, Integer\n",
    "    - Surname - Surname of the customer, Character\n",
    "    - CreditScore - a number between 300â€“850 that depicts a consumer's creditworthiness, Integer\n",
    "    - Geography - City to which customers belongs, Character\n",
    "    - Gender - Gender of the customer, Character\n",
    "    - Age - Age of the customer, Integer\n",
    "    - Tenure - Tenure of the customer with a bank, Integer\n",
    "    - Balance - Balance held by the customer, Float\n",
    "    - NumOfProducts - Number of bank services used by the customer, Integer\n",
    "    - HasCrCard - Customer has a credit card or not, Integer, Binary\n",
    "    - IsActiveMember - Customer is an active member or not, Integer, Binary\n",
    "    - EstimatedSalary - Estimated salary of the customer, Float\n",
    "    - Exited - If the Customer is going to exit the bank or not, Integer, Binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e0e36",
   "metadata": {
    "id": "102e0e36"
   },
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "Perform the data preprocessing that is required for the data that you have downloaded. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdebf8",
   "metadata": {
    "id": "06fdebf8"
   },
   "source": [
    "## 3.1 Apply techiniques\n",
    "\n",
    "* to remove duplicate data\n",
    "* to impute or remove missing data\n",
    "* to remove data inconsistencies\n",
    "\n",
    "IF ANY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3118eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dd3118eb",
    "outputId": "38c7eedb-ea61-4458-8db1-90591f667981"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "## Check duplicate data, if there is any, we can use Numpy unique() function.\n",
    "\n",
    "data.duplicated().sum()\n",
    "## it is seen that there are no duplicates in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RbriDAQO8ioY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RbriDAQO8ioY",
    "outputId": "ba66ebec-e606-480f-9a36-7a1041173afb"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "## To impute or remove missing data - check for isnull first. If there is data missing,\n",
    "## then depending on the data different methods can be used including Imputation of missing values, or imputing with additional column.\n",
    "data.isnull().sum()\n",
    "## here we see that isnull shows all data good, same as the results from head() above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ca738",
   "metadata": {
    "id": "cb3ca738"
   },
   "source": [
    "## 3.2 Encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc7ffc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "08fc7ffc",
    "outputId": "8dfaee26-cf0e-4fca-b613-1e442c0b42a4"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "#Generating Dependent Variable Vectors\n",
    "X = data.iloc[:,3:-1].values\n",
    "Y = data.iloc[:,-1].values\n",
    "\n",
    "#Encoding Categorical Variable Gender\n",
    "LE1 = LabelEncoder()\n",
    "X[:,2] = np.array(LE1.fit_transform(X[:,2]))\n",
    "\n",
    "#Encoding Categorical variable Geography\n",
    "ct =ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3897985",
   "metadata": {
    "id": "d3897985"
   },
   "source": [
    "## 3.3 Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7856104",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "c7856104",
    "outputId": "b6f41206-e71c-4bd8-c3bf-48cb03c94d46"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "## Machine learning algorithms tend to perform better or converge faster when the different features (variables) are on a smaller scale. \n",
    "## Therefore it is common practice to normalize the data before training machine learning models on it.\n",
    "\n",
    "## We can either use normalize() or MinMaxScaler()\n",
    "## We are normalizing the data set here, for certain columns without converting individual columns to arrays.\n",
    "X = normalize(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc0a45",
   "metadata": {
    "id": "90bc0a45"
   },
   "source": [
    "## 3.4 Feature Engineering \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9412d5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "f9412d5c",
    "outputId": "c6d5306e-4453-4357-ada9-e8604c726056"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "## 1. We try to visualize the Exited Data and plot charts\n",
    "data['Exited'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fIHIDT4nUYe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "fIHIDT4nUYe4",
    "outputId": "e140c3a3-6861-4cf3-8f0b-a364e02c2dec"
   },
   "outputs": [],
   "source": [
    "# Exited and Non-Exited Customers visualization\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.countplot(x='Exited',data=data)\n",
    "plt.title(\"% Exited and Non-Exited Customers\")\n",
    "plt.subplot(1,2,2)\n",
    "labels =data['Exited'].value_counts(sort = True).index\n",
    "sizes = data['Exited'].value_counts(sort = True)\n",
    "plt.pie(sizes,labels=[\"Not-Exited\",\"Exited\"],autopct='%1.2f%%', shadow=True, startangle=90)\n",
    "plt.title('% Exited and Non-Exited People',size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ygK4NKXNU4V9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "ygK4NKXNU4V9",
    "outputId": "9558bf18-e2ab-4058-9f25-2c6162f27bf8"
   },
   "outputs": [],
   "source": [
    "#statistical info of data set\n",
    "data.describe().T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cd04b",
   "metadata": {
    "id": "793cd04b"
   },
   "source": [
    "## 3.5 Identify the target variables.\n",
    "\n",
    "* Separate the data from the target such that the dataset is in the form of (X,y) or (Features, Label)\n",
    "\n",
    "* Discretize / Encode the target variable or perform one-hot encoding on the target or any other as and if required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9089b57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "c9089b57",
    "outputId": "2b33955f-b0e5-4491-c965-24e6e82140c5"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "## This Dataset contains details records of customers and his/her bank details \n",
    "## and output/target varibale consists of whether the customer left the bank (closed his account) or he continues to be a customer.\n",
    "\n",
    "# Visualize the Numerical features\n",
    "fig, ax = plt.subplots(2, 2, figsize = (14, 8))\n",
    "sns.boxplot(x = 'Exited', y = 'CreditScore', data = data, ax = ax[0][0],palette='twilight')\n",
    "sns.boxplot(x = 'Exited', y = 'Age', data = data, ax = ax[0][1],palette='twilight')\n",
    "sns.boxplot(x = 'Exited', y = 'Balance', data = data, ax = ax[1][0],palette='twilight')\n",
    "sns.boxplot(x = 'Exited', y = 'EstimatedSalary', data = data, ax = ax[1][1],palette='twilight')\n",
    "\n",
    "ax[0][0].set_title('Credit-Score Data',color='brown',fontsize=15)\n",
    "ax[0][1].set_title('Age of Customers',color='brown',fontsize=15)\n",
    "ax[1][0].set_title('Balance Amount',color='brown',fontsize=15)\n",
    "ax[1][1].set_title('Salary Amount',color='brown',fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd14601",
   "metadata": {
    "id": "4cd14601"
   },
   "source": [
    "## 3.6 Split the data into training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74cd9c",
   "metadata": {
    "id": "1a74cd9c"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "#Splitting dataset into training and testing dataset\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "\n",
    "#Performing Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cec4fc",
   "metadata": {
    "id": "e3cec4fc"
   },
   "source": [
    "## 3.7 Report\n",
    "\n",
    "Mention the method adopted  and justify why the method was used\n",
    "* to remove duplicate data, if present \n",
    "* to impute or remove missing data, if present \n",
    "* to remove data inconsistencies, if present \n",
    "* to encode categorical data \n",
    "* the normalization technique used\n",
    "\n",
    "If the any of the above are not present, then also add in the report below.\n",
    "\n",
    "Report the size of the training dataset and testing dataset\n",
    "\n",
    "Score: 3 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187aeb95",
   "metadata": {
    "id": "187aeb95"
   },
   "source": [
    "##---------Type the answer below this line------------------##\n",
    "\n",
    "\n",
    "a. Checking Duplicated Data\n",
    "\n",
    "-  we used duplicated() and found no duplicate data. If there was duplicate data, we can use Numpy unique() function to remove it. \n",
    "\n",
    "b. Checking for Null values\n",
    "\n",
    "- we used isnull() and also the info() above showed no Null values. If there is data missing, then depending on the data different methods can be used including Imputation of missing values, or imputing with additional column.\n",
    "\n",
    "c. Inconsistencies\n",
    "\n",
    " - from info() above, we see that all 10000 rows are consistent in the data set.\n",
    "\n",
    "d. Encode Categorical data:\n",
    "  - For gender we used LabelEncoder to Encode target labels with value between 0 and n_classes-1.\n",
    "  - For geograhy, we used ColumnTransformer and OneHotEncoder to Encode categorical features as a one-hot numeric array.\n",
    "\n",
    "e. Normalize Data:\n",
    "- we used normalize() to normalize the data set here, for certain columns without converting individual columns to arrays. We could also use MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0b5d2",
   "metadata": {
    "id": "3ae0b5d2"
   },
   "source": [
    "# 4. Deep Neural Network Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bf4d7",
   "metadata": {
    "id": "186bf4d7"
   },
   "source": [
    "## 4.1 Design the architecture that you will be using to solve the prediction problem identified.\n",
    "\n",
    "* Add dense layers, specifying the number of units in each layer and the activation function used in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d7b27",
   "metadata": {
    "id": "868d7b27"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "#Initialising ANN and its Architecture\n",
    "\n",
    "# As per the Assignment statement, Using keras for constructing Deep Neural Network architecture + Model buildingÂ \n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "#Adding First Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=6,activation=\"relu\"))\n",
    "\n",
    "#Adding Second Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=6,activation=\"relu\"))\n",
    "\n",
    "#Adding Output Layer\n",
    "ann.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f9e37",
   "metadata": {
    "id": "575f9e37"
   },
   "source": [
    "## 4.2 Report\n",
    "\n",
    "Report the following and provide justification for the same.\n",
    "\n",
    "* Number of layers\n",
    "* Number of units in each layer\n",
    "* Activation function used in each hidden layer\n",
    "* Activation function used in the output layer\n",
    "* Total number of trainable parameters \n",
    "\n",
    "Score: 4 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a15d8",
   "metadata": {
    "id": "a95a15d8"
   },
   "source": [
    "##---------Type the answer below this line------------------##\n",
    "\n",
    "\n",
    "a. Number of layers\n",
    "- We are adding 3 layers here - 2 hidden and one sigmoid\n",
    "\n",
    "b. Number of units in each layer\n",
    "- 6 units each in the hidden layers and 1 in the Sigmoid layer\n",
    "\n",
    "c. Activation function used in each hidden layer\n",
    "- relu. RELU returns 0 if the x (input) is less than 0\n",
    "\n",
    "d. Activation function used in the output layer\n",
    "- sigmoid. Sigmoid function returns the value beteen 0 and 1\n",
    "\n",
    "e. Total number of trainable parameters\n",
    "- There are 127 Trainable parameters in this model (as per summary() below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc82a1",
   "metadata": {
    "id": "bdbc82a1"
   },
   "source": [
    "# 5. Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca214eb3",
   "metadata": {
    "id": "ca214eb3"
   },
   "source": [
    "## 5.1 Configure the training\n",
    "\n",
    "Configure  the model for training, by using appropriate optimizers and regularizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e9754",
   "metadata": {
    "id": "a85e9754"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "#Compiling using Optimizer on ANN\n",
    "ann.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "#Regularizations using EarlyStopping\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping             \n",
    "#create callback : -\n",
    "cb=EarlyStopping(\n",
    "    monitor=\"val_loss\",  #val_loss refers to the testing error\n",
    "    min_delta=0.00001, #value of the lambda \n",
    "    patience=15,\n",
    "    verbose=1,\n",
    "    mode=\"auto\", #minimize loss and #maximize accuracy\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd60d8",
   "metadata": {
    "id": "32fd60d8"
   },
   "source": [
    "## 5.2 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efaa227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2680
    },
    "id": "8efaa227",
    "outputId": "d44dc0b2-064d-4df7-d069-7efc00124cea"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "#Fitting ANN\n",
    "trained_ann = ann.fit(X_train, Y_train, batch_size=32, epochs = 100, \n",
    "                      callbacks=cb, \n",
    "                      validation_data=(X_test,Y_test))\n",
    "\n",
    "ann.summary()\n",
    "\n",
    "plot_model(ann,\n",
    "           to_file=\"model.png\",\n",
    "           show_shapes=True,\n",
    "           show_layer_names=True,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yaANGa27rF9Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "yaANGa27rF9Z",
    "outputId": "b1bebf1a-0803-4e25-ce1b-0402faa043dd"
   },
   "outputs": [],
   "source": [
    "trained_ann.history['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd0c56",
   "metadata": {
    "id": "19bd0c56"
   },
   "source": [
    "Justify your choice of optimizers and regulizations used and the hyperparameters tuned\n",
    "\n",
    "Score: 4 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70064645",
   "metadata": {
    "id": "70064645"
   },
   "source": [
    "##---------Type the answers below this line------------------##\n",
    "\n",
    "### Choice of optimizers and regularizations used and the hyperparameters tuned\n",
    "1. Optimizer used => Adam\n",
    "- Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models.\n",
    "- The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.\n",
    "- We used Adam because it can be configured easily, its effective and  it achieves good results fast.\n",
    "\n",
    "2. Regularization method used => Early Stopping for training neural networks in the choice of the number of training epochs to use.\n",
    "- Early stopping is a method that allows you to specify an arbitrarily large number of training epochs and stop training once the model performance stops improving on the validation dataset.\n",
    "- We use Early Stopping to Halt the Training of Neural Networks At the Right Time\n",
    "- Also, this method reduces overfitting by adding an early stopping to an existing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1173c",
   "metadata": {
    "id": "06f1173c"
   },
   "source": [
    "# 6. Test the model\n",
    "\n",
    "Score: 2 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042235d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7042235d",
    "outputId": "7b25d2ae-f3af-482b-b41c-79f51e9d47ad"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "ann.evaluate(X_train,Y_train)\n",
    "print(\"Training accuracy :\",ann.evaluate(X_train,Y_train)[1])\n",
    "print(\"Training loss :\",ann.evaluate(X_train,Y_train)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EVzoZJDltnx1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "EVzoZJDltnx1",
    "outputId": "a90101b7-7ed6-4b97-ed02-59865b35aa6c"
   },
   "outputs": [],
   "source": [
    "ann.evaluate(X_test,Y_test)\n",
    "print(\"Testing accuracy is:\",ann.evaluate(X_test,Y_test)[1])\n",
    "print(\"Testing loss is:\",ann.evaluate(X_test,Y_test)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57940c",
   "metadata": {
    "id": "eb57940c"
   },
   "source": [
    "## 7. Conclusion \n",
    "\n",
    "Plot the training and validation loss\n",
    "Report the testing accuracy and loss.\n",
    "\n",
    "Report values for preformance study metrics like accuracy, precision, recall, F1 Score.\n",
    "\n",
    "A proper comparision based on different metrics should be done and not just accuracy alone, only then the comparision becomes authentic. You may use Confusion matrix, classification report, MAE etc per the requirement of your application/problem.\n",
    "\n",
    "Score 2 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smtynANytwDg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "smtynANytwDg",
    "outputId": "7bd17590-26cb-4e8d-a208-a25defdb0782"
   },
   "outputs": [],
   "source": [
    "## To Plot the training and validation loss Report the testing accuracy and loss.\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(trained_ann.history['accuracy'],label='Training accuracy',color='blue')\n",
    "plt.plot(trained_ann.history['val_accuracy'],label='Testing accuracy',color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('No of Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbncWg6lyQBr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "bbncWg6lyQBr",
    "outputId": "0ed268d0-b8ff-4c21-972a-cfe2327d72e5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(trained_ann.history['loss'],label='Training loss',color='red')\n",
    "plt.plot(trained_ann.history['val_loss'],label='Testing loss',color='green')\n",
    "plt.legend()\n",
    "plt.xlabel('No of Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8NxZczxSyW9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8NxZczxSyW9e",
    "outputId": "d5ba409a-caff-4584-e623-5480359f35cd"
   },
   "outputs": [],
   "source": [
    "y_pred_prob=ann.predict(X_test)\n",
    "y_pred=np.where(y_pred_prob>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sQuw16V1ychB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "sQuw16V1ychB",
    "outputId": "b498e156-5f95-4825-9a13-1cd005e657c2"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jtzs8c7h-zdF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Jtzs8c7h-zdF",
    "outputId": "5a4818fa-9f2e-4715-ac45-533b76ea13ac"
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,y_pred))\n",
    "print(confusion_matrix(Y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nfE4PMFF-2Pn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "nfE4PMFF-2Pn",
    "outputId": "a10928bb-db02-4a0f-9e98-3a23bab0155c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(confusion_matrix(Y_test,y_pred),annot=True,cmap=\"OrRd_r\",\n",
    "            fmt=\"d\",cbar=True,\n",
    "            annot_kws={\"fontsize\":15})\n",
    "plt.xlabel(\"Actual Result\")\n",
    "plt.ylabel(\"Predicted Result\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H5FZMFTv_O3C",
   "metadata": {
    "id": "H5FZMFTv_O3C"
   },
   "source": [
    "##7. Steps completed:\n",
    "- Plotted the training and validation loss\n",
    "- Reported the testing accuracy and loss.\n",
    "- Reported values for preformance study metrics like accuracy, precision, recall, F1 Score.\n",
    "- Apart from accuracy, used confusion matrix too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf06eb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "9bf06eb1",
    "outputId": "5dd14466-e343-418a-8094-36bae0fd20ba"
   },
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "#Predicting result for Single Observation\n",
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1,50000]])) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed0137",
   "metadata": {
    "id": "79ed0137"
   },
   "source": [
    "## 8. Solution\n",
    "\n",
    "What is the solution that is proposed to solve the business problem discussed in Section 1. Also share your learnings while working through solving the problem in terms of challenges, observations, decisions made etc.\n",
    "\n",
    "### What is the Solution proposed here?\n",
    "- We have proposed a Prediction model using ANN to solve the Customer Churn Modelling.\n",
    "\n",
    "### Why is this Prediction important?\n",
    "\n",
    "####For a bank, this prediction is very much significant as\n",
    "- Acquiring new customers often costs more than retaining existing ones. \n",
    "-\tOnce customers at risk of churn are identified, the decision can be taken on marketing strategies can be taken to maximize customers likelihood of continuing with the services of the bank.\n",
    "\n",
    "\n",
    "### Challenges, Observations, Decisions Made\n",
    "- Observation 1 - For such finance problems, ANNs helps draw conclusions from case observations and address the issues of prediction and interpretation.\n",
    "- Observation 2 - ANN can learn from events (exit data here) and make decisions based on the observations.\n",
    "- Observations 3 - For small data sets such as this churn data are employed, deep learning models can easily overfit and learn to associate each training input with the correct label.\n",
    "- Challenge 1 - The main challenges for a neural network are learning speed and training set size. In order to generalize well, a neural network must have enough training data.\n",
    "- Challenge 2 - Challenges with optimization and accuracy\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
